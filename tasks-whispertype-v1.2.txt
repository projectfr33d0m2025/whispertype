# WhisperType v1.2 - Implementation Tasks

## Overview
This document breaks down the WhisperType v1.2 implementation into phases and actionable tasks.
Check off tasks as they are completed.

**Version:** 1.2.0
**Target Duration:** 8 weeks
**Major Features:**
1. Processing Modes (Writing Improvement + Auto-Format)
2. LLM Integration (Local-first with optional cloud)
3. Custom Vocabulary System
4. App-Aware Context System

---

## Architecture Note: Independent Testability

Each phase is designed to be **independently testable** without requiring subsequent phases.
This is achieved through:

1. **Protocol-based design**: Interfaces defined early, implementations added later
2. **Stub implementations**: Placeholder implementations allow testing before real code exists
3. **Feature flags**: New features can be disabled to test core functionality
4. **Clear boundaries**: Each phase produces testable artifacts

---

## Phase 1: Core Processing Pipeline (Week 1-2)

### Phase 1 Testability
After completing Phase 1, you can test:
- ✅ Raw, Clean, and Formatted modes (fully functional)
- ✅ Polished and Professional modes (fall back to Formatted, show "AI unavailable" indicator)
- ✅ Filler removal with 20+ test cases
- ✅ Settings persistence
- ✅ Processing Settings UI
- ✅ End-to-end transcription flow

**What's stubbed:** LLMEngine protocol exists but returns `.unavailable` status.

---

### 1.1 Processing Mode Foundation
- [x] Create ProcessingMode.swift enum with 5 modes:
  - raw, clean, formatted, polished, professional
- [x] Add computed properties:
  - displayName: String
  - description: String
  - icon: String (emoji)
  - requiresLLM: Bool
- [x] Add Codable conformance for persistence
- [x] Add CaseIterable for UI enumeration
- [x] Write unit tests for ProcessingMode (manual testing completed)

### 1.2 Settings Model Updates
- [x] Add processingMode: ProcessingMode to AppSettings (default: .formatted)
- [x] Add fillerRemovalEnabled: Bool (default: true)
- [x] Add llmPreference: LLMPreference enum (default: .localFirst)
- [x] Add ollamaModel: String (default: "llama3.2:3b")
- [x] Add ollamaHost: String (default: "localhost")
- [x] Add ollamaPort: Int (default: 11434)
- [x] Add UserDefaults persistence for all new settings
- [x] Write unit tests for settings persistence (manual testing completed)

### 1.3 Filler Removal Implementation
- [x] Create FillerRemover.swift class
- [x] Define primary filler patterns (regex):
  - um, umm, uh, uhh, uhm, er, erm, ah, ahh, hmm, hm
- [x] Define contextual filler patterns:
  - "like" (when filler, not comparison)
  - "you know" (standalone/sentence-end)
  - "sort of", "kind of", "I mean", "basically"
- [x] Implement false start detection:
  - Pattern: "X— Y" where X and Y start similarly
- [x] Implement remove(_ text: String) -> String
- [x] Handle edge cases:
  - Preserve "um" in words like "umbrella"
  - Preserve "like" in comparisons
- [x] Write comprehensive unit tests (manual testing with various phrases)

### 1.4 Basic Formatting Rules
- [x] Create FormattingRules.swift class
- [x] Implement capitalizeFirstLetter()
- [x] Implement capitalizeAfterPunctuation()
- [x] Implement capitalizeI() - standalone "i" → "I"
- [x] Implement normalizeWhitespace()
- [x] Implement normalizeEllipsis() - "..." consistency
- [x] Implement apply(_ text: String) -> String
- [x] Write unit tests for formatting rules (manual testing completed)

### 1.5 LLM Engine Protocol (Stub for Phase 2)
- [x] Create LLMEngineProtocol.swift:
  ```swift
  protocol LLMEngineProtocol {
      var status: LLMEngineStatus { get async }
      func process(_ text: String, mode: ProcessingMode, context: TranscriptionContext) async throws -> String
  }
  ```
- [x] Create LLMEngineStatus enum:
  - available(provider: String)
  - unavailable(reason: String)
  - processing
- [x] Create StubLLMEngine.swift:
  - Always returns `.unavailable(reason: "AI enhancement not configured")`
  - process() throws LLMError.notAvailable
- [x] Write tests for stub behavior (verified via console logs)

### 1.6 Post-Processor Orchestration
- [x] Create PostProcessor.swift class
- [x] Inject LLMEngineProtocol via initializer (dependency injection)
- [x] Define processing chain based on mode:
  - raw: pass-through
  - clean: FillerRemover only
  - formatted: FillerRemover + FormattingRules
  - polished: FillerRemover + FormattingRules + LLM (if available)
  - professional: FillerRemover + FormattingRules + LLM (if available)
- [x] Implement process(_ text: String, mode: ProcessingMode, context: TranscriptionContext) async -> String
- [x] **Fallback logic**: If LLM unavailable for polished/professional, use formatted processing
- [x] Return tuple: (processedText: String, actualModeUsed: ProcessingMode)
- [x] Add timing/performance logging
- [x] Write integration tests with StubLLMEngine (manual end-to-end testing)

### 1.7 Processing Settings UI
- [x] Create ProcessingSettingsView.swift (SwiftUI)
- [x] Add Processing tab to SettingsView
- [x] Implement mode selection with visual cards:
  - Icon, name, description for each mode
  - "Recommended" badge on Formatted
  - "AI Required" badge on Polished/Professional
  - **"AI Not Configured" warning** when LLM unavailable and AI mode selected
- [x] Add filler removal toggle with description
- [x] Add "AI Enhancement Engine" section:
  - Show current status (Stub shows "Not Configured")
  - Placeholder text: "Configure in Phase 2"
- [x] Add navigation to Vocabulary (placeholder, disabled)
- [x] Add navigation to App Rules (placeholder, disabled)
- [x] Bind all controls to AppSettings
- [x] Test settings persistence and UI state (manual testing completed)

### 1.8 Integration with Transcription Flow
- [x] Update AppCoordinator to read processingMode from settings
- [x] Inject PostProcessor with StubLLMEngine into transcription pipeline
- [x] Update transcription flow:
  1. Whisper transcribes audio
  2. PostProcessor processes based on mode
  3. TextInjector inserts processed text
- [x] Update RecordingOverlayContentView to show:
  - Current mode name
  - "AI unavailable" indicator if mode requires LLM but unavailable
  (Note: Currently shows mode via settings, AI indicator deferred to Phase 2)
- [x] Test end-to-end flow with all 5 modes (polished/professional fall back)

### 1.9 Phase 1 Verification Tests
- [x] Unit test: FillerRemover removes all filler patterns (manual testing)
- [x] Unit test: FormattingRules capitalizes correctly (manual testing)
- [x] Unit test: PostProcessor chains correctly for each mode (manual testing)
- [x] Unit test: PostProcessor falls back when LLM unavailable (manual testing)
- [x] Integration test: Settings persist across app restart (manual testing)
- [x] Integration test: Full transcription flow with formatted mode (manual testing)

---

## Phase 2: LLM Integration (Week 3-4)

### Phase 2 Testability
After completing Phase 2, you can test:
- ✅ Ollama connection and model detection
- ✅ OpenAI API integration
- ✅ LLM fallback chain (local → cloud → basic)
- ✅ All 5 processing modes with real LLM enhancement
- ✅ Rate limiting and error handling
- ✅ First launch AI setup flow

**Dependencies:** Requires Phase 1 completion (PostProcessor, ProcessingMode).

---

### 2.1 LLM Provider Protocol
- [x] Create LLMProvider.swift protocol:
  - id: String
  - displayName: String
  - isLocal: Bool
  - status: LLMProviderStatus (async)
  - process(_ request: LLMRequest) async throws -> LLMResponse
  - cancel()
- [x] Create LLMProviderStatus enum:
  - available, unavailable(reason), connecting, rateLimit(retryAfter)
- [x] Create LLMRequest struct:
  - text, mode, context, timeout
- [x] Create LLMResponse struct:
  - processedText, processingTime, tokensUsed, providerUsed
- [x] Create LLMError enum:
  - rateLimited, invalidAPIKey, networkError, timeout, modelNotFound, notAvailable

### 2.2 Prompt Builder
- [x] Create PromptBuilder.swift class
- [x] Implement buildSystemPrompt(mode: ProcessingMode) -> String
- [x] Implement buildUserPrompt(text: String, context: TranscriptionContext) -> String
- [x] Define prompts for each mode:
  - Clean: Remove fillers only
  - Formatted: Fillers + punctuation + capitalization
  - Polished: Grammar + clarity
  - Professional: Formal tone + complete sentences
- [x] Add placeholder for vocabulary injection (used in Phase 3):
  - Method signature: setVocabularyTerms(_ terms: [String])
  - Currently no-op, stores terms for later use
- [x] Write tests verifying prompt structure

### 2.3 Ollama Provider Implementation
- [x] Create OllamaProvider.swift class implementing LLMProvider
- [x] Implement configuration:
  - host, port, model, timeout
- [x] Implement status check (GET /api/tags)
- [x] Implement model list retrieval
- [x] Implement process() via POST /api/generate:
  - Build request body (model, prompt, options)
  - Set temperature: 0.1, top_p: 0.9
  - Parse response
- [x] Implement cancel() to abort request
- [x] Handle connection errors gracefully
- [x] Handle timeout (default: 10s)
- [x] Write unit tests with mock HTTP

### 2.4 Ollama Model Detection
- [x] Create OllamaModelManager.swift class
- [x] Define known model ratings dictionary:
  - llama3.2:1b, llama3.2:3b, llama3.1:8b, mistral:7b, etc.
  - Speed rating (1-5), Quality rating (1-5)
- [x] Implement detectInstalledModels() async -> [ModelInfo]
- [x] Implement recommendModel(from: [ModelInfo]) -> ModelInfo?
- [x] Prioritization logic:
  1. llama3.2:3b if installed
  2. Any 3B model with speed >= 4
  3. Any 7-8B model
  4. First available
- [x] Write tests for recommendation logic

### 2.5 OpenAI Provider Implementation
- [x] Create CloudProvider.swift class implementing LLMProvider (supports OpenAI and OpenRouter)
- [x] Implement API key storage in Keychain:
  - Save key securely
  - Retrieve key
  - Delete key
- [x] Implement status check (verify key exists)
- [x] Implement process() via POST /v1/chat/completions:
  - Model: gpt-4o-mini (or OpenRouter equivalent)
  - Build messages array (system + user)
  - Set temperature: 0.1, max_tokens: 500
  - Parse response
- [x] Handle rate limiting (429 response):
  - Parse Retry-After header
  - Throw rateLimited error
- [x] Handle authentication errors (401)
- [x] Write unit tests with mock HTTP

### 2.6 LLM Engine Implementation
- [x] Create LLMEngine.swift class implementing LLMEngineProtocol
- [x] Replace StubLLMEngine in PostProcessor with real LLMEngine
- [x] Implement provider management:
  - localProvider: OllamaProvider?
  - cloudProvider: CloudProvider?
- [x] Implement LLMPreference enum:
  - localOnly, localFirst, cloudFirst, cloudOnly, disabled
- [x] Implement process() with fallback logic:
  1. Determine provider order from preference
  2. Try first available provider
  3. On failure, try next provider
  4. On all failures, throw error (PostProcessor handles fallback)
- [x] Publish currentStatus for UI binding
- [x] Publish lastProviderUsed for transparency
- [x] Implement setupProviders() from settings
- [x] Write integration tests

### 2.7 LLM Settings UI
- [x] Update ProcessingSettingsView "AI Enhancement Engine" section
- [x] Create provider selection picker (LLMPreference)
- [x] Show Ollama connection status:
  - Connected (green)
  - Not Running (yellow)
  - Error (red)
- [x] Add Ollama model picker (from detected models)
- [x] Add "Configure API Key" button for cloud providers
- [x] Create APIKeyInputSheet for secure key entry
- [x] Show masked API key if configured
- [x] Test all UI states

### 2.8 Fallback Notifications
- [x] Create ToastNotificationWindow.swift for floating toast overlay
- [x] Implement floating toast that appears at top center of screen
- [x] Toast auto-dismisses after duration (3-4 seconds)
- [x] Toast has slide-in/fade-out animation
- [x] Toast supports dismiss button
- [x] Implement fallback notification in transcribeAudio():
  - "AI enhancement unavailable. Using [mode] mode."
  - Orange/warning styling
  - Auto-dismiss after 3 seconds
- [x] Implement rate limit notification:
  - "Rate limited. Switched to [mode] mode."
  - Auto-dismiss after 4 seconds
- [x] Track wasRateLimited in ProcessingResult
- [x] Integrate floating toast with PostProcessor result

### 2.9 First Launch AI Setup
- [ ] Update FirstLaunchView with AI setup step (SKIPPED - simplified for now)
- [ ] Implement Ollama detection flow:
  - Show "Detecting local AI..." spinner
  - If found: Show model name, "Your transcriptions stay private"
  - If not found: Show installation instructions
- [ ] Add options:
  - [Use Local AI] - Set preference to localFirst
  - [Configure Cloud] - Open cloud settings
  - [Skip Enhancement] - Set preference to disabled
- [ ] Test all first-launch paths

### 2.10 Menu Bar Updates
- [x] Add processingSection to MenuBarView
- [x] Show current processing mode with icon:
  - Mode name (Raw, Clean, Formatted, Polished, Professional)
  - Mode submenu to change mode
- [x] Show AI engine status:
  - Engine name (Ollama, OpenRouter, etc.)
  - Status indicator (green/red/orange/yellow dot)
  - Preference subtitle (Local only, Local preferred, etc.)
- [x] Add LLMEngine as @ObservedObject to MenuBarView
- [x] Mode submenu disables LLM modes when unavailable

### 2.11 Phase 2 Verification Tests
- [x] Unit test: OllamaProvider connects and generates
- [x] Unit test: CloudProvider handles rate limits
- [x] Unit test: LLMEngine falls back correctly
- [x] Integration test: Polished mode produces enhanced text
- [x] Integration test: Professional mode produces formal text
- [x] Integration test: Fallback notification appears when LLM unavailable

---

## Phase 3: Vocabulary System (Week 5-6)

### Phase 3 Testability
Phase 3 is split into two parts for independent testability:

**Part A (3.1-3.4, 3.7-3.12): Vocabulary Core** - Can be tested independently
- ✅ Vocabulary CRUD operations
- ✅ Storage and persistence
- ✅ Fuzzy matching correction
- ~~✅ Auto-learning detection~~ (SKIPPED - deferred indefinitely)
- ✅ Full vocabulary UI

**Part B (3.5-3.6): Vocabulary Integrations** - Requires Phase 1+2
- ✅ Whisper initial_prompt hints
- ✅ LLM prompt vocabulary injection

---

### Part A: Vocabulary Core (Independent)

### 3.1 Vocabulary Data Model
- [x] Create VocabularyEntry.swift struct:
  - id: UUID
  - term: String
  - phonetic: String?
  - aliases: [String]
  - ~~contexts: [String]~~ (deferred to Phase 4)
  - source: VocabularySource (manual, imported)
  - isPinned: Bool
  - useCount: Int
  - createdAt: Date
  - lastUsed: Date?
- [x] Add Codable conformance
- [x] Add Identifiable conformance
- [x] Add Hashable conformance
- [x] Added VocabularySortOrder enum for sorting
- [x] Added CSV export/import helpers

### 3.2 Vocabulary Storage
- [x] Create VocabularyStorage.swift class
- [x] Define storage path: ~/Library/Application Support/WhisperType/vocabulary.json
- [x] Implement load() -> [VocabularyEntry]
- [x] Implement save(_ entries: [VocabularyEntry])
- [x] Implement atomic writes (write to temp, rename)
- [x] Implement exportToCSV() and importFromCSV()
- [x] Added fileSizeBytes, fileSizeFormatted, fileExists helpers

### 3.3 Vocabulary Manager Core
- [x] Create VocabularyManager.swift ObservableObject
- [x] Define maxEntries: 200
- [x] Implement CRUD operations:
  - add(_ entry: VocabularyEntry) throws
  - update(_ entry: VocabularyEntry)
  - delete(_ id: UUID)
  - get(_ id: UUID) -> VocabularyEntry?
- [x] Implement search via searchQuery property
- [x] Implement sorting via sortOrder property
- [x] Validate entry count limit on add
- [x] Publish entries for UI binding
- [x] Added VocabularyError enum for error handling
- [x] Added ImportMergeStrategy and ImportResult for imports

### 3.4 Vocabulary Prioritization
- [x] Implement getWhisperHints() -> [String]:
  - Top 20 entries by useCount (pinned first)
  - Return term strings only
- [x] Implement getLLMVocabulary() -> [String]:
  - All pinned entries first
  - Fill remaining with top usage (total max: 50)
- [x] Implement incrementUsage(_ term: String):
  - Update useCount and lastUsed

### 3.7 Post-Processing Fuzzy Matching
- [x] Create VocabularyCorrector.swift class
- [x] Implement Levenshtein distance calculation
- [x] Implement correct(_ text: String, vocabulary: [VocabularyEntry]) -> CorrectionResult:
  - Tokenize text into words
  - For each word, check against all entries + aliases
  - If distance <= 2 and entry exists, replace
- [x] Handle case preservation
- [x] Handle punctuation preservation
- [x] Added Correction struct for tracking corrections
- [x] Added CorrectionResult struct with corrections list

### 3.8 Auto-Learning Detection
- ~~SKIPPED~~ - Auto-learning feature deferred indefinitely per user request

### 3.9 Vocabulary Settings UI - Main View
- [x] Create VocabularySettingsView.swift (SwiftUI)
- [x] Implemented as new tab in SettingsContainerView
- [x] Add header:
  - Entry count display: "X / 200 entries"
  - Search field
  - Sort picker
  - Add button
- [x] Empty state with helpful guidance
- [x] Import/Export buttons in footer
- [x] Storage info display

### 3.10 Vocabulary Settings UI - Entry List
- [x] Create VocabularyEntryRow.swift component
- [x] Display entry info:
  - Term (bold)
  - Phonetic (if exists)
  - Source badge (manual/imported)
  - Usage count
  - Pin indicator
- [x] Add row actions (on hover):
  - Edit button
  - Delete button
  - Pin/unpin toggle
- [x] Context menu with same actions
- [x] Implement sections:
  - Pinned (always active)
  - All (sortable)

### 3.11 Vocabulary Entry Editor
- [x] Create VocabularyEntryEditorView.swift
- [x] Implement as sheet/modal
- [x] Form fields:
  - Term (required)
  - Phonetic (optional)
  - Aliases (comma-separated)
  - ~~Contexts (multi-select)~~ (deferred to Phase 4)
  - Pinned toggle
- [x] Add validation:
  - Term not empty
  - Term not duplicate
- [x] Save/Cancel buttons
- [x] Supports both Add and Edit modes

### 3.12 Vocabulary Import/Export
- [x] Implement exportToCSV() -> URL in VocabularyManager
- [x] Implement importFromCSV(_ url: URL, mergeStrategy:) in VocabularyManager
- [x] Add Export button to VocabularySettingsView
- [x] Add Import button with file picker
- [x] Handle import conflicts (3 merge strategies)
- [x] Import result summary dialog

### Part A Verification Tests
- [x] VocabularyManager CRUD operations (manual testing)
- [x] 200 entry limit enforced
- [x] VocabularyCorrector fuzzy matching
- [x] ~~Auto-learning suggestion detection~~ (SKIPPED)
- [x] Vocabulary UI add/edit/delete (manual testing)
- [x] Import/export round-trip (manual testing)

---

### Part B: Vocabulary Integrations (Requires Phase 1+2)

### 3.5 Whisper Integration
- [x] WhisperWrapper.transcribe() already accepts vocabulary parameter
- [x] whisper_full_params already sets initial_prompt
- [x] Update AppCoordinator to get hints from VocabularyManager
- [x] Inject vocabulary hints into transcription call

### 3.6 LLM Vocabulary Injection
- [x] PromptBuilder.setVocabularyTerms() already implemented
- [x] Vocabulary terms appended to prompts as "Important terms to spell correctly"
- [x] Update PostProcessor to fetch vocabulary and set in PromptBuilder

### 3.13 Vocabulary in Processing Pipeline
- [x] Add VocabularyCorrector to PostProcessor chain:
  - Runs after FillerRemover
  - Runs before FormattingRules
- [x] PostProcessor fetches vocabulary from VocabularyManager
- [x] Increment vocabulary usage after successful correction
- [x] ProcessingResult includes vocabularyCorrections list

### Part B Verification Tests
- [x] Integration test: Whisper uses vocabulary hints (manual testing)
- [x] Integration test: LLM prompt includes vocabulary (manual testing)
- [x] Integration test: Full pipeline with vocabulary correction (manual testing)

---

## Phase 4: App-Aware Context (Week 7)

### Phase 4 Testability
After completing Phase 4, you can test:
- ✅ App detection via NSWorkspace
- ✅ Default presets for 18 apps
- ✅ Custom rule creation and persistence
- ✅ Mode switching based on frontmost app
- ✅ App Rules UI

**Dependencies:** Requires Phase 1 completion (ProcessingMode).
**Optional Enhancement:** If Phase 2 complete, menu bar shows LLM status per app.

**NOTE FROM PHASE 3:** Remember to implement VocabularyEntry.contexts field 
integration here - vocabulary entries should be filterable by app context.

---

### 4.1 Context Detection
- [ ] Create ContextDetector.swift class
- [ ] Implement detectFrontmostApp() -> AppInfo:
  - Use NSWorkspace.shared.frontmostApplication
  - Extract bundleIdentifier and localizedName
- [ ] Create AppInfo struct:
  - bundleIdentifier: String
  - name: String
- [ ] Handle nil/unknown app gracefully
- [ ] Write tests with mock workspace

### 4.2 App Preset Model
- [ ] Create AppPreset.swift struct:
  - bundleIdentifier: String
  - displayName: String
  - defaultMode: ProcessingMode
  - rationale: String
- [ ] Create AppRule.swift struct (user overrides):
  - bundleIdentifier: String
  - mode: ProcessingMode
  - isCustom: Bool
- [ ] Add Codable conformance
- [ ] Write unit tests

### 4.3 Default Presets Data
- [ ] Create DefaultAppPresets.swift with static data
- [ ] Add all 18 presets:
  - Development: Terminal, VS Code, Xcode, Sublime Text
  - Email: Mail, Outlook
  - Messaging: Slack, Discord, Teams, WhatsApp
  - Browsers: Safari, Chrome
  - Documents: Notes, Word, Pages, TextEdit
  - Spreadsheets: Excel, Numbers
- [ ] Write validation tests (no duplicate bundle IDs)

### 4.4 App-Aware Manager
- [ ] Create AppAwareManager.swift ObservableObject
- [ ] Implement storage: ~/Library/Application Support/WhisperType/app-rules.json
- [ ] Implement getModeForApp(_ bundleId: String) -> ProcessingMode:
  1. Check user custom rules
  2. Check default presets
  3. Return global default
- [ ] Implement setCustomRule(_ bundleId: String, mode: ProcessingMode)
- [ ] Implement removeCustomRule(_ bundleId: String)
- [ ] Implement resetAllRules()
- [ ] Add appAwarenessEnabled: Bool setting
- [ ] Publish rules for UI binding
- [ ] Write comprehensive tests

### 4.5 Integration with Processing Pipeline
- [ ] Update AppCoordinator to use ContextDetector
- [ ] Get current app on recording start
- [ ] Query AppAwareManager for appropriate mode
- [ ] Override global mode with app-specific mode (if enabled)
- [ ] Log app detection for debugging
- [ ] Test with various apps

### 4.6 Recording Overlay Updates
- [ ] Update RecordingOverlayContentView to show:
  - Current mode
  - App-specific indicator: "(for VS Code)" when different from default
- [ ] Add small app icon or name hint (optional)
- [ ] Test overlay with app switching

### 4.7 App Rules Settings UI - Main View
- [ ] Create AppRulesSettingsView.swift (SwiftUI)
- [ ] Implement as separate window
- [ ] Add header:
  - Enable/disable app-awareness toggle
- [ ] Add app list with:
  - App icon (from bundle)
  - App name
  - Current mode (dropdown)
  - Source indicator (default/custom)
- [ ] Add "Reset to default" per-app action
- [ ] Write layout tests

### 4.8 App Rules Settings UI - Add App
- [ ] Create AddAppSheet.swift
- [ ] Implement "Select from running apps" option:
  - List currently running apps
  - Filter out system apps
- [ ] Implement "Browse Applications" option:
  - File picker for /Applications
  - Extract bundle info
- [ ] Add mode selection
- [ ] Validate against existing rules
- [ ] Test both app selection methods

### 4.9 App Rules Import/Export
- [ ] Implement exportRules() -> URL (JSON)
- [ ] Implement importRules(_ url: URL) throws
- [ ] Add Export/Import buttons to UI
- [ ] Handle import conflicts
- [ ] Test round-trip

### 4.10 Menu Bar App-Aware Indicator
- [ ] Update menu bar to show app-aware mode:
  - "Mode: Formatted (VS Code)" when app-specific
  - "Mode: Formatted" when using default
- [ ] Enable "Use App-Aware Mode" toggle in submenu
- [ ] Test indicator updates on app switch

### 4.11 Phase 4 Verification Tests
- [ ] Unit test: ContextDetector returns correct app info
- [ ] Unit test: AppAwareManager priority logic (custom > preset > default)
- [ ] Unit test: All 18 presets have unique bundle IDs
- [ ] Integration test: Mode switches when changing apps
- [ ] Integration test: Custom rules persist across restart

---

## Phase 5: Polish & Release (Week 8)

### Phase 5 Testability
Phase 5 is integration and release work. All previous phases must be complete.

---

### 5.1 Enable All Features
- [ ] Enable Vocabulary navigation in Processing Settings (was placeholder)
- [ ] Enable App Rules navigation in Processing Settings (was placeholder)
- [ ] Connect all components in AppCoordinator:
  - LLMEngine (real, not stub)
  - VocabularyManager
  - AppAwareManager
- [ ] Verify all feature flags are enabled

### 5.2 UI Polish
- [ ] Review all new UI for consistency
- [ ] Verify dark mode appearance
- [ ] Verify light mode appearance
- [ ] Check all icon sizes and alignments
- [ ] Verify all animations are smooth
- [ ] Check all error messages for clarity

### 5.3 Documentation Updates
- [ ] Update README.md:
  - Add v1.2 features section
  - Add Processing Modes documentation
  - Add Vocabulary documentation
  - Add App-Aware documentation
  - Add LLM setup guide
- [ ] Add screenshots of new UI
- [ ] Update troubleshooting section
- [ ] Review and update FAQ

### 5.4 Code Cleanup
- [ ] Remove debug logging
- [ ] Remove StubLLMEngine (replaced by real LLMEngine)
- [ ] Review TODOs and FIXMEs
- [ ] Ensure consistent code style
- [ ] Add missing documentation comments
- [ ] Review error handling coverage

### 5.5 Release Preparation
- [ ] Update version to 1.2.0 in Xcode project
- [ ] Update RELEASE_NOTES.md
- [ ] Build Release configuration
- [ ] Code sign application
- [ ] Create DMG with updated background

### 5.6 GitHub Release
- [ ] Create v1.2.0 tag
- [ ] Write release notes with:
  - Feature highlights
  - Breaking changes (if any)
  - Known issues
  - Upgrade instructions
- [ ] Upload DMG
- [ ] Publish release

---

## Implementation Notes

### Processing Mode Constants
```swift
enum ProcessingMode: String, Codable, CaseIterable {
    case raw
    case clean
    case formatted
    case polished
    case professional
    
    var requiresLLM: Bool {
        switch self {
        case .raw, .clean, .formatted: return false
        case .polished, .professional: return true
        }
    }
    
    var displayName: String {
        switch self {
        case .raw: return "Raw"
        case .clean: return "Clean"
        case .formatted: return "Formatted"
        case .polished: return "Polished"
        case .professional: return "Professional"
        }
    }
}
```

### LLM Engine Protocol (Phase 1 Stub)
```swift
protocol LLMEngineProtocol: AnyObject {
    var status: LLMEngineStatus { get async }
    func process(_ text: String, mode: ProcessingMode, context: TranscriptionContext) async throws -> String
}

enum LLMEngineStatus {
    case available(provider: String)
    case unavailable(reason: String)
    case processing
}

// Stub implementation for Phase 1
class StubLLMEngine: LLMEngineProtocol {
    var status: LLMEngineStatus {
        get async { .unavailable(reason: "AI enhancement not configured") }
    }
    
    func process(_ text: String, mode: ProcessingMode, context: TranscriptionContext) async throws -> String {
        throw LLMError.notAvailable
    }
}
```

### Filler Word Regex Patterns
```swift
// Primary fillers (always remove)
let primaryFillers = #/\b(uh+m*|um+|er+m?|ah+|hm+)\b/#

// Contextual fillers (remove when standalone)
let contextualFillers = #/\b(you know|I mean|like,?\s+(you know)?|sort of|kind of|basically)\b/#

// False starts (remove first part)
let falseStarts = #/(\b\w+\b)\s*[—–-]\s*/#
```

### Ollama API Endpoints
```
GET  http://localhost:11434/api/tags      # List installed models
POST http://localhost:11434/api/generate  # Generate completion

Request body:
{
  "model": "llama3.2:3b",
  "prompt": "...",
  "stream": false,
  "options": {
    "temperature": 0.1,
    "top_p": 0.9,
    "num_predict": 500
  }
}
```

### OpenAI API Endpoint
```
POST https://api.openai.com/v1/chat/completions
Headers:
  Authorization: Bearer <API_KEY>
  Content-Type: application/json

Request body:
{
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."}
  ],
  "temperature": 0.1,
  "max_tokens": 500
}
```

### Vocabulary Storage Path
```swift
let vocabularyURL = FileManager.default
    .urls(for: .applicationSupportDirectory, in: .userDomainMask)
    .first!
    .appendingPathComponent("WhisperType")
    .appendingPathComponent("vocabulary.json")
```

### App Bundle Identifiers Reference
```swift
// Development
"com.apple.Terminal"
"com.microsoft.VSCode"
"com.apple.dt.Xcode"
"com.sublimetext.4"

// Email
"com.apple.mail"
"com.microsoft.Outlook"

// Messaging
"com.tinyspeck.slackmacgap"
"com.hnc.Discord"
"com.microsoft.teams2"
"net.whatsapp.WhatsApp"

// Browsers
"com.apple.Safari"
"com.google.Chrome"

// Documents
"com.apple.Notes"
"com.microsoft.Word"
"com.apple.iWork.Pages"
"com.apple.TextEdit"

// Spreadsheets
"com.microsoft.Excel"
"com.apple.iWork.Numbers"
```

### Levenshtein Distance Implementation
```swift
func levenshteinDistance(_ s1: String, _ s2: String) -> Int {
    let s1 = Array(s1.lowercased())
    let s2 = Array(s2.lowercased())
    var dist = [[Int]](repeating: [Int](repeating: 0, count: s2.count + 1), count: s1.count + 1)
    
    for i in 0...s1.count { dist[i][0] = i }
    for j in 0...s2.count { dist[0][j] = j }
    
    for i in 1...s1.count {
        for j in 1...s2.count {
            let cost = s1[i-1] == s2[j-1] ? 0 : 1
            dist[i][j] = min(
                dist[i-1][j] + 1,      // deletion
                dist[i][j-1] + 1,      // insertion
                dist[i-1][j-1] + cost  // substitution
            )
        }
    }
    return dist[s1.count][s2.count]
}
```

---

## Estimated Effort by Phase

| Phase | Description | Duration | Tasks |
|-------|-------------|----------|-------|
| 1 | Core Processing Pipeline | 2 weeks | 38 tasks |
| 2 | LLM Integration | 2 weeks | 48 tasks |
| 3A | Vocabulary Core | 1.5 weeks | 35 tasks |
| 3B | Vocabulary Integrations | 0.5 weeks | 10 tasks |
| 4 | App-Aware Context | 1 week | 29 tasks |
| 5 | Polish & Release | 1 week | 18 tasks |
| **Total** | | **8 weeks** | **178 tasks** |

---

## Dependencies

### External Dependencies
- whisper.cpp (existing)
- HotKey (existing)
- Ollama (user-installed, optional)
- OpenAI API (user-configured, optional)

### Internal Dependencies (Updated for Testability)
```
Phase 1 ──────────────────────────────────────────┐
    │                                              │
    │ Creates: ProcessingMode, PostProcessor,      │
    │          LLMEngineProtocol (stub)            │
    │                                              │
    ▼                                              │
Phase 2 ──────────────────────────────────────────┤
    │                                              │
    │ Creates: Real LLMEngine, Providers           │
    │ Replaces: StubLLMEngine                      │
    │                                              │
    ▼                                              │
Phase 3A (Vocabulary Core) ◄───────────────────────┼─── Can run in parallel!
    │                                              │
    │ Independent: VocabularyManager, UI           │
    │                                              │
    ▼                                              │
Phase 3B (Vocabulary Integrations) ◄───────────────┘
    │
    │ Requires: Phase 1 + 2
    │
    ▼
Phase 4 (App-Aware) ◄─── Requires Phase 1 only
    │
    ▼
Phase 5 (Polish) ◄─── Requires all previous
```

### Build Dependencies
- macOS 13.0+ SDK
- Xcode 15+
- Swift 5.9+

---

## Risk Mitigation Checkpoints

### End of Phase 1
- [ ] All 5 modes selectable in UI
- [ ] Raw, Clean, Formatted work fully
- [ ] Polished, Professional gracefully fall back to Formatted
- [ ] UI shows "AI not configured" when LLM modes selected
- [ ] Filler removal accuracy > 90%
- [ ] Settings persist correctly
- [ ] No performance regression from v1.1

### End of Phase 2
- [ ] Ollama integration stable
- [ ] Fallback behavior reliable (local → cloud → basic)
- [ ] Cloud integration optional and secure
- [ ] < 2s latency with local LLM
- [ ] All 5 modes work with real enhancement

### End of Phase 3A
- [ ] Vocabulary CRUD works correctly
- [ ] 200 entry limit enforced
- [ ] Import/export works
- [ ] UI handles large lists smoothly

### End of Phase 3B
- [ ] Whisper uses vocabulary hints
- [ ] LLM prompts include vocabulary
- [ ] Post-processing corrects based on vocabulary

### End of Phase 4
- [ ] App detection is reliable
- [ ] Mode switching is seamless
- [ ] All 18 presets are correct
- [ ] Custom rules persist correctly

---

## Manual Testing Checklist (User)

This section is for manual testing by the developer. Not tracked as implementation tasks.

### End-to-End Testing
- [ ] Test complete flow: Record → Transcribe → Process → Inject
- [ ] Test all 5 processing modes with real speech
- [ ] Test LLM fallback scenarios:
  - [ ] Ollama not running
  - [ ] OpenAI rate limited
  - [ ] Network offline
- [ ] Test vocabulary integration at all points
- [ ] Test app-aware mode switching between apps

### Performance Testing
- [ ] Measure transcription + processing latency
  - Target: < 2 seconds for local LLM
  - Target: < 500ms for cloud LLM
- [ ] Profile memory usage (idle, during transcription, with vocabulary)
- [ ] Profile CPU usage during LLM processing

### Edge Cases
- [ ] Very long transcriptions (5+ minutes)
- [ ] Empty/silent audio
- [ ] Rapid successive recordings
- [ ] 200 vocabulary entries loaded
- [ ] 18+ app rules configured
- [ ] Settings changes during recording
- [ ] App switching during recording

### Multi-Monitor
- [ ] Recording overlay positioning
- [ ] Different monitor arrangements
- [ ] External display as primary
- [ ] Hot-plugging displays

### Accessibility
- [ ] VoiceOver compatibility for new UI
- [ ] Keyboard navigation in settings
- [ ] Focus management in modals
- [ ] Reduced motion preference

### Installation & Upgrade
- [ ] Fresh install from DMG
- [ ] Upgrade from v1.1 (settings migration)
- [ ] First launch setup flow
- [ ] Permissions prompts work correctly

---

## Notes

- All new features should have feature flags for gradual rollout
- Prioritize local-first experience in all UX decisions
- Keep privacy messaging consistent throughout
- Test thoroughly on both Intel and Apple Silicon Macs
- Consider beta release after Phase 4 for early feedback
- Phase 3A can be developed in parallel with Phase 2 for faster delivery
